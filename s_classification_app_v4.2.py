#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¥æ„Ÿå½±åƒç›‘ç£åˆ†ç±»ç³»ç»Ÿ - Streamlit Webç‰ˆ v4.2
==========================================
ä¿®å¤å¤šåˆ†ç±»å™¨å¾ªç¯æ‰§è¡Œé—®é¢˜
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import seaborn as sns
import geopandas as gpd
import rioxarray as rxr
import xarray as xr
from rasterio import features
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, 
                              AdaBoostClassifier, ExtraTreesClassifier)
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.kernel_approximation import Nystroem, RBFSampler
from sklearn.pipeline import Pipeline
from sklearn.metrics import (confusion_matrix, accuracy_score, classification_report, 
                           cohen_kappa_score, precision_score, recall_score, f1_score)
import warnings
warnings.filterwarnings('ignore')

import streamlit as st
from pathlib import Path
import tempfile
import base64
import zipfile
import shutil
import gc

sns.set_style("whitegrid")

# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.sans-serif"] = ["SimHei", "DejaVu Sans", "Arial Unicode MS"]
plt.rcParams["axes.unicode_minus"] = False

# æ£€æŸ¥å¯é€‰åº“
try:
    import openpyxl
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False
    st.warning("âš ï¸ æœªå®‰è£…openpyxlï¼Œå°†æ— æ³•å¯¼å‡ºExcelæ–‡ä»¶")

# ==================== åç«¯å¤„ç†ç±» ====================
class ClassificationBackend:
    """åˆ†ç±»å¤„ç†åç«¯"""
    
    def __init__(self):
        self.RANDOM_STATE = 42
        
        # é¢„å®šä¹‰é¢œè‰²
        self.LANDUSE_COLORS = {
            "æ°´ä½“": "lightblue", "æ²³æµ": "blue", "æ¹–æ³Š": "deepskyblue",
            "æ¤è¢«": "forestgreen", "æ£®æ—": "darkgreen", "è‰åœ°": "limegreen",
            "å†œç”°": "yellowgreen", "è€•åœ°": "olivedrab",
            "å»ºç­‘": "gray", "åŸå¸‚": "dimgray", "å±…æ°‘åœ°": "slategray",
            "è£¸åœ°": "tan", "æ²™åœ°": "wheat", "å…¶ä»–": "darkred"
        }
        
        self.COLOR_PALETTE = ['forestgreen', 'lightblue', 'gray', 'tan', 'yellow', 
                             'darkred', 'purple', 'orange', 'pink', 'brown']
        
        # æ£€æŸ¥å¯é€‰åº“
        self.check_optional_libraries()
    
    def check_optional_libraries(self):
        """æ£€æŸ¥å¯é€‰åº“æ˜¯å¦å¯ç”¨"""
        self.has_xgboost = False
        self.has_lightgbm = False
        
        try:
            import xgboost
            from xgboost import XGBClassifier
            _ = XGBClassifier(n_estimators=10, verbosity=0)
            self.has_xgboost = True
        except Exception:
            pass
        
        try:
            import lightgbm
            from lightgbm import LGBMClassifier
            _ = LGBMClassifier(n_estimators=10, verbose=-1)
            self.has_lightgbm = True
        except Exception:
            pass
    
    def get_all_classifiers(self, n_estimators=100, fast_mode=False, n_train_samples=None):
        """è·å–æ‰€æœ‰å¯ç”¨åˆ†ç±»å™¨"""
        if fast_mode:
            n_est = min(50, n_estimators)
            max_depth = 10
            max_iter = 200
        else:
            n_est = n_estimators
            max_depth = 20
            max_iter = 500
        
        if n_train_samples:
            n_components = min(1000, n_train_samples // 2)
        else:
            n_components = 1000
        
        classifiers = {
            "rf": (RandomForestClassifier(n_estimators=n_est, n_jobs=-1, random_state=self.RANDOM_STATE, 
                                         verbose=0, max_depth=max_depth, min_samples_split=5, 
                                         max_features='sqrt'),
                  "éšæœºæ£®æ—", "Random Forest", False, False, "fast"),
            
            "et": (ExtraTreesClassifier(n_estimators=n_est, n_jobs=-1, random_state=self.RANDOM_STATE,
                                       verbose=0, max_depth=max_depth, min_samples_split=5, max_features='sqrt'),
                  "æç«¯éšæœºæ ‘", "Extra Trees", False, False, "fast"),
            
            "dt": (DecisionTreeClassifier(random_state=self.RANDOM_STATE, max_depth=max_depth,
                                         min_samples_split=5, min_samples_leaf=2),
                  "å†³ç­–æ ‘", "Decision Tree", False, False, "very_fast"),
            
            "svm_linear": (SVC(kernel="linear", C=1.0, cache_size=500, probability=True, 
                             random_state=self.RANDOM_STATE, max_iter=max_iter),
                          "SVM-çº¿æ€§æ ¸", "SVM Linear", False, True, "medium"),
            
            "linear_svc": (CalibratedClassifierCV(LinearSVC(C=1.0, max_iter=max_iter, random_state=self.RANDOM_STATE,
                                                           dual=False, loss='squared_hinge'), cv=3),
                          "çº¿æ€§SVM(å¿«)", "Linear SVM", False, True, "fast"),
            
            "sgd_svm": (SGDClassifier(loss='hinge', penalty='l2', max_iter=max_iter, n_jobs=-1,
                                     random_state=self.RANDOM_STATE, learning_rate='optimal'),
                       "SGD-SVM", "SGD SVM", False, True, "very_fast"),
            
            "nystroem_svm": (Pipeline([
                ("feature_map", Nystroem(kernel='rbf', gamma=0.1, n_components=n_components, 
                                        random_state=self.RANDOM_STATE)),
                ("sgd", SGDClassifier(max_iter=max_iter, random_state=self.RANDOM_STATE))
            ]), "æ ¸è¿‘ä¼¼SVM", "Nystroem SVM", False, True, "fast"),
            
            "rbf_sampler_svm": (Pipeline([
                ("feature_map", RBFSampler(gamma=0.1, n_components=n_components, random_state=self.RANDOM_STATE)),
                ("sgd", SGDClassifier(max_iter=max_iter, random_state=self.RANDOM_STATE))
            ]), "RBFé‡‡æ ·SVM", "RBF Sampler", False, True, "fast"),
            
            "svm_rbf": (SVC(kernel="rbf", C=1.0, gamma='scale', cache_size=500, probability=True, 
                          random_state=self.RANDOM_STATE),
                       "SVM-RBFæ ¸âš ï¸", "SVM RBF", False, True, "very_slow"),
            
            "knn": (KNeighborsClassifier(n_neighbors=5, n_jobs=-1, algorithm='ball_tree', leaf_size=30),
                   "Kè¿‘é‚»", "KNN", False, True, "slow"),
            
            "nb": (GaussianNB(), "æœ´ç´ è´å¶æ–¯", "Naive Bayes", False, False, "very_fast"),
            
            "gb": (GradientBoostingClassifier(n_estimators=n_est, learning_rate=0.1, max_depth=5,
                                             random_state=self.RANDOM_STATE, verbose=0, subsample=0.8),
                  "æ¢¯åº¦æå‡", "Gradient Boosting", False, False, "medium"),
            
            "ada": (AdaBoostClassifier(n_estimators=n_est, learning_rate=1.0, 
                                      random_state=self.RANDOM_STATE, algorithm='SAMME.R'),
                   "AdaBoost", "AdaBoost", False, False, "medium"),
            
            "lr": (LogisticRegression(max_iter=max_iter, n_jobs=-1, random_state=self.RANDOM_STATE,
                                     verbose=0, solver='lbfgs', multi_class='multinomial'),
                  "é€»è¾‘å›å½’", "Logistic Regression", False, True, "very_fast"),
            
            "mlp": (MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=max_iter, random_state=self.RANDOM_STATE,
                                 verbose=False, early_stopping=True, validation_fraction=0.1, 
                                 n_iter_no_change=10, learning_rate='adaptive'),
                   "ç¥ç»ç½‘ç»œ", "MLP", False, True, "medium"),
        }
        
        if self.has_xgboost:
            try:
                from xgboost import XGBClassifier
                classifiers["xgb"] = (
                    XGBClassifier(n_estimators=n_est, learning_rate=0.1, max_depth=6, n_jobs=-1,
                                 random_state=self.RANDOM_STATE, verbosity=0, tree_method='hist',
                                 subsample=0.8, colsample_bytree=0.8),
                    "XGBoost", "XGBoost", True, False, "fast"
                )
            except Exception:
                pass
        
        if self.has_lightgbm:
            try:
                from lightgbm import LGBMClassifier
                classifiers["lgb"] = (
                    LGBMClassifier(n_estimators=n_est, learning_rate=0.1, max_depth=max_depth, n_jobs=-1,
                                  random_state=self.RANDOM_STATE, verbose=-1, num_leaves=31,
                                  subsample=0.8, colsample_bytree=0.8, force_col_wise=True),
                    "LightGBM", "LightGBM", False, False, "very_fast"
                )
            except Exception:
                pass
        
        return classifiers
    
    def get_background_mask(self, image, background_value=0):
        """è·å–èƒŒæ™¯æ©è†œ"""
        data = image.values
        if background_value == 0:
            background_mask = np.all(data == 0, axis=0)
        else:
            background_mask = np.all(data == background_value, axis=0)
        return background_mask
    
    def get_shapefile_fields(self, shp_path):
        """è·å–shapefileçš„æ‰€æœ‰å­—æ®µå"""
        try:
            gdf = gpd.read_file(shp_path)
            return list(gdf.columns)
        except Exception as e:
            st.error(f"è¯»å–shapefileå­—æ®µå¤±è´¥: {e}")
            return []
    
    def get_class_info_from_shp(self, shp_path, class_attr, name_attr):
        """ä»shpæ–‡ä»¶è·å–ç±»åˆ«ä¿¡æ¯"""
        gdf = gpd.read_file(shp_path)
        
        if name_attr not in gdf.columns or name_attr == class_attr:
            gdf[name_attr] = gdf[class_attr].apply(lambda x: f"ç±»åˆ«_{x}")
        
        class_info = gdf[[class_attr, name_attr]].drop_duplicates()
        class_names = dict(zip(class_info[class_attr], class_info[name_attr]))
        
        class_colors = {}
        for i, (class_id, class_name) in enumerate(class_names.items()):
            color_found = False
            for key, color in self.LANDUSE_COLORS.items():
                if key in str(class_name):
                    class_colors[class_id] = color
                    color_found = True
                    break
            if not color_found:
                class_colors[class_id] = self.COLOR_PALETTE[i % len(self.COLOR_PALETTE)]
        
        return class_names, class_colors, sorted(class_names.keys())
    
    def rasterize_samples(self, shp, ref_img, attr):
        """çŸ¢é‡æ …æ ¼åŒ–"""
        gdf = gpd.read_file(shp)
        gdf = gdf.to_crs(ref_img.rio.crs)
        shapes = ((geom, value) for geom, value in zip(gdf.geometry, gdf[attr]))
        
        arr = features.rasterize(shapes=shapes, out_shape=ref_img.shape[1:],
                                transform=ref_img.rio.transform(), fill=0,
                                all_touched=True, dtype="uint16")
        return arr
    
    def extract_samples(self, image, mask, ignore_background=True, background_value=0, max_samples=None):
        """æå–æ ·æœ¬"""
        data = np.moveaxis(image.values, 0, -1)
        valid = mask > 0
        
        if ignore_background:
            background_mask = self.get_background_mask(image, background_value)
            valid = valid & (~background_mask)
        
        X = data[valid]
        y = mask[valid]
        
        # æ¸…ç†NaNå’ŒInf
        nan_mask = np.isnan(X).any(axis=1)
        inf_mask = np.isinf(X).any(axis=1)
        bad_mask = nan_mask | inf_mask
        
        n_nan = np.sum(nan_mask)
        n_inf = np.sum(inf_mask)
        
        X = X[~bad_mask]
        y = y[~bad_mask]
        
        # æ£€æŸ¥æ ·æœ¬æ•°é‡
        if len(y) == 0:
            return X, y, n_nan, n_inf, 0
        
        # åˆ†å±‚é‡‡æ ·
        n_sampled = 0
        if max_samples is not None and len(y) > max_samples:
            n_original = len(y)
            unique_classes = np.unique(y)
            
            if len(unique_classes) > 1:
                splitter = StratifiedShuffleSplit(n_splits=1, train_size=max_samples, 
                                                 random_state=self.RANDOM_STATE)
                sample_idx, _ = next(splitter.split(X, y))
                X = X[sample_idx]
                y = y[sample_idx]
                n_sampled = n_original - len(y)
            else:
                np.random.seed(self.RANDOM_STATE)
                sample_idx = np.random.choice(len(y), max_samples, replace=False)
                X = X[sample_idx]
                y = y[sample_idx]
                n_sampled = n_original - len(y)
        
        return X, y, n_nan, n_inf, n_sampled
    
    def calculate_metrics(self, y_true, y_pred):
        """è®¡ç®—è¯„ä»·æŒ‡æ ‡"""
        return {
            'overall_accuracy': accuracy_score(y_true, y_pred),
            'kappa': cohen_kappa_score(y_true, y_pred),
            'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),
            'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),
            'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),
        }
    
    def estimate_prediction_time(self, clf_code, n_pixels, speed_tag):
        """ä¼°ç®—é¢„æµ‹æ—¶é—´"""
        time_per_million = {"very_fast": 1, "fast": 3, "medium": 10, "slow": 30, "very_slow": 300}
        base_time = time_per_million.get(speed_tag, 10)
        return (n_pixels / 1_000_000) * base_time
    
    def predict_by_block(self, model, image, out_path, block_size=512, 
                        ignore_background=True, background_value=0, progress_callback=None,
                        label_encoder=None, scaler=None):
        """åˆ†å—é¢„æµ‹"""
        height, width = image.shape[1], image.shape[2]
        prediction = np.zeros((height, width), dtype='uint16')
        
        if ignore_background:
            background_mask = self.get_background_mask(image, background_value)
        
        total_blocks = int(np.ceil(height / block_size))
        
        for i, y in enumerate(range(0, height, block_size)):
            h = min(block_size, height - y)
            block_data = image.isel(y=slice(y, y+h)).values
            data = np.moveaxis(block_data, 0, -1)
            original_shape = data.shape
            data_flat = data.reshape(-1, data.shape[-1])
            
            if ignore_background:
                block_bg_mask = background_mask[y:y+h, :].flatten()
                non_bg_indices = ~block_bg_mask
                
                if np.any(non_bg_indices):
                    data_to_predict = np.nan_to_num(data_flat[non_bg_indices], 
                                                   nan=0.0, posinf=0.0, neginf=0.0)
                    
                    if scaler is not None:
                        data_to_predict = scaler.transform(data_to_predict)
                    
                    preds_non_bg = model.predict(data_to_predict)
                    
                    if label_encoder is not None:
                        preds_non_bg = label_encoder.inverse_transform(preds_non_bg)
                    
                    preds_flat = np.zeros(len(data_flat), dtype='uint16')
                    preds_flat[non_bg_indices] = preds_non_bg
                    preds = preds_flat.reshape(original_shape[0], original_shape[1])
                else:
                    preds = np.zeros((original_shape[0], original_shape[1]), dtype='uint16')
            else:
                data_flat = np.nan_to_num(data_flat, nan=0.0, posinf=0.0, neginf=0.0)
                
                if scaler is not None:
                    data_flat = scaler.transform(data_flat)
                
                preds = model.predict(data_flat)
                
                if label_encoder is not None:
                    preds = label_encoder.inverse_transform(preds)
                
                preds = preds.reshape(original_shape[0], original_shape[1]).astype("uint16")
            
            prediction[y:y+h, :] = preds
            
            if progress_callback:
                progress_callback((i + 1) / total_blocks)
        
        # ä¿å­˜ç»“æœ
        prediction_da = xr.DataArray(prediction, dims=['y', 'x'],
                                     coords={'y': image.coords['y'], 'x': image.coords['x']})
        
        prediction_da.rio.write_crs(image.rio.crs, inplace=True)
        prediction_da.rio.write_transform(image.rio.transform(), inplace=True)
        prediction_da.rio.write_nodata(background_value, inplace=True)
        
        prediction_da.rio.to_raster(out_path, driver='GTiff', dtype='uint16', 
                                    compress='lzw', tiled=True)
        return out_path

# ==================== è¾…åŠ©å‡½æ•° ====================
def extract_zip_file(zip_file, extract_dir):
    """è§£å‹ZIPæ–‡ä»¶åˆ°æŒ‡å®šç›®å½•"""
    with zipfile.ZipFile(zip_file, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    
    # æŸ¥æ‰¾.shpæ–‡ä»¶
    shp_files = list(Path(extract_dir).glob("*.shp"))
    if not shp_files:
        raise ValueError("ZIPæ–‡ä»¶ä¸­æœªæ‰¾åˆ°.shpæ–‡ä»¶")
    
    return str(shp_files[0])

def save_uploaded_file(uploaded_file, temp_dir, filename):
    """ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•"""
    file_path = os.path.join(temp_dir, filename)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return file_path

def safe_delete_temp_dir(temp_dir):
    """å®‰å…¨åˆ é™¤ä¸´æ—¶ç›®å½•ï¼Œå¤„ç†æ–‡ä»¶å ç”¨é—®é¢˜"""
    if not temp_dir or not os.path.exists(temp_dir):
        return
    
    max_retries = 3
    for i in range(max_retries):
        try:
            # å…ˆå°è¯•æ­£å¸¸åˆ é™¤
            shutil.rmtree(temp_dir)
            break
        except PermissionError:
            if i < max_retries - 1:
                # ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
                time.sleep(1)
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
            else:
                # æœ€åä¸€æ¬¡å°è¯•ï¼Œå¿½ç•¥é”™è¯¯
                try:
                    shutil.rmtree(temp_dir, ignore_errors=True)
                except:
                    pass

def add_log(message):
    """æ·»åŠ æ—¥å¿—æ¶ˆæ¯"""
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    st.session_state.log_messages.append(message)
    # ä¿æŒæ—¥å¿—é•¿åº¦åˆç†
    if len(st.session_state.log_messages) > 100:
        st.session_state.log_messages = st.session_state.log_messages[-50:]

# ==================== Streamlitåº”ç”¨ ====================
def main():
    st.set_page_config(
        page_title="é¥æ„Ÿå½±åƒç›‘ç£åˆ†ç±»ç³»ç»Ÿ v4.2 - Webç‰ˆ",
        page_icon="ğŸ›°ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # åˆå§‹åŒ–session state
    if 'backend' not in st.session_state:
        st.session_state.backend = ClassificationBackend()
    
    if 'is_running' not in st.session_state:
        st.session_state.is_running = False
    
    if 'comparison_results' not in st.session_state:
        st.session_state.comparison_results = []
    
    if 'comparison_df' not in st.session_state:
        st.session_state.comparison_df = None
    
    if 'log_messages' not in st.session_state:
        st.session_state.log_messages = []
    
    if 'progress' not in st.session_state:
        st.session_state.progress = 0
    
    # æ–°å¢ï¼šåˆ†ç±»å™¨æ‰§è¡ŒçŠ¶æ€
    if 'current_classifier_index' not in st.session_state:
        st.session_state.current_classifier_index = 0
    
    if 'selected_classifiers_list' not in st.session_state:
        st.session_state.selected_classifiers_list = []
    
    if 'classification_params' not in st.session_state:
        st.session_state.classification_params = {}
    
    # æ ‡é¢˜å’Œä»‹ç»
    st.title("ğŸ›°ï¸ é¥æ„Ÿå½±åƒç›‘ç£åˆ†ç±»ç³»ç»Ÿ v4.2 - Webç‰ˆ")
    st.markdown("""
    ### ä¸“ä¸šçº§é¥æ„Ÿå½±åƒåˆ†ç±»å·¥å…·
    
    åŸºäºæœºå™¨å­¦ä¹ çš„å¤šç®—æ³•å¯¹æ¯”åˆ†ç±»ç³»ç»Ÿï¼Œæ”¯æŒ15+ç§åˆ†ç±»å™¨ï¼Œæä¾›å®Œæ•´çš„ç²¾åº¦è¯„ä¼°å’Œå¯è§†åŒ–åˆ†æã€‚
    
    **æ³¨æ„**: 
    - Shapefileè¯·æ‰“åŒ…ä¸ºZIPæ ¼å¼ä¸Šä¼ ï¼ŒåŒ…å«.shp, .shx, .dbf, .prjç­‰æ‰€æœ‰å¿…éœ€æ–‡ä»¶
    - ç¡®ä¿è®­ç»ƒæ ·æœ¬ä¸å½±åƒæ–‡ä»¶åœ¨ç›¸åŒçš„åœ°ç†åæ ‡ç³»ä¸‹
    - å¦‚æœé‡åˆ°æ ·æœ¬æå–é—®é¢˜ï¼Œè¯·æ£€æŸ¥èƒŒæ™¯å€¼è®¾ç½®æ˜¯å¦æ­£ç¡®
    """)
    
    # ä¾§è¾¹æ  - å‚æ•°è®¾ç½®
    st.sidebar.header("ğŸ“‹ å‚æ•°è®¾ç½®")
    
    # æ–‡ä»¶ä¸Šä¼ 
    st.sidebar.subheader("ğŸ“ æ•°æ®æ–‡ä»¶")
    image_file = st.sidebar.file_uploader("é¥æ„Ÿå½±åƒæ–‡ä»¶", type=['tif', 'tiff'], key="image")
    train_zip_file = st.sidebar.file_uploader("è®­ç»ƒæ ·æœ¬ZIPæ–‡ä»¶", type=['zip'], key="train_zip",
                                             help="è¯·ä¸Šä¼ åŒ…å«.shp, .shx, .dbf, .prjç­‰æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…")
    
    val_zip_file = st.sidebar.file_uploader("éªŒè¯æ ·æœ¬ZIPæ–‡ä»¶ (å¯é€‰)", type=['zip'], key="val_zip",
                                           help="è¯·ä¸Šä¼ åŒ…å«.shp, .shx, .dbf, .prjç­‰æ–‡ä»¶çš„ZIPå‹ç¼©åŒ…")
    
    # å­—æ®µé…ç½®
    st.sidebar.subheader("ğŸ·ï¸ å­—æ®µé…ç½®")
    
    # å¦‚æœä¸Šä¼ äº†è®­ç»ƒæ ·æœ¬ZIPæ–‡ä»¶ï¼Œå¤„ç†å¹¶è·å–å­—æ®µ
    if train_zip_file and 'train_shp_path' not in st.session_state:
        with st.spinner("æ­£åœ¨è§£å‹è®­ç»ƒæ ·æœ¬æ–‡ä»¶..."):
            try:
                # åˆ›å»ºä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp()
                train_zip_path = save_uploaded_file(train_zip_file, temp_dir, "train_samples.zip")
                
                # è§£å‹ZIPæ–‡ä»¶
                extract_dir = os.path.join(temp_dir, "train_extracted")
                os.makedirs(extract_dir, exist_ok=True)
                
                train_shp_path = extract_zip_file(train_zip_path, extract_dir)
                st.session_state.train_shp_path = train_shp_path
                st.session_state.train_temp_dir = temp_dir
                
                st.sidebar.success("è®­ç»ƒæ ·æœ¬è§£å‹æˆåŠŸ!")
                
            except Exception as e:
                st.sidebar.error(f"è§£å‹è®­ç»ƒæ ·æœ¬å¤±è´¥: {str(e)}")
    
    # å¦‚æœä¸Šä¼ äº†éªŒè¯æ ·æœ¬ZIPæ–‡ä»¶ï¼Œå¤„ç†å¹¶è·å–å­—æ®µ
    if val_zip_file and 'val_shp_path' not in st.session_state:
        with st.spinner("æ­£åœ¨è§£å‹éªŒè¯æ ·æœ¬æ–‡ä»¶..."):
            try:
                # åˆ›å»ºä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp()
                val_zip_path = save_uploaded_file(val_zip_file, temp_dir, "val_samples.zip")
                
                # è§£å‹ZIPæ–‡ä»¶
                extract_dir = os.path.join(temp_dir, "val_extracted")
                os.makedirs(extract_dir, exist_ok=True)
                
                val_shp_path = extract_zip_file(val_zip_path, extract_dir)
                st.session_state.val_shp_path = val_shp_path
                st.session_state.val_temp_dir = temp_dir
                
                st.sidebar.success("éªŒè¯æ ·æœ¬è§£å‹æˆåŠŸ!")
                
            except Exception as e:
                st.sidebar.error(f"è§£å‹éªŒè¯æ ·æœ¬å¤±è´¥: {str(e)}")
    
    # è·å–å­—æ®µåˆ—è¡¨
    if 'train_shp_path' in st.session_state:
        fields = st.session_state.backend.get_shapefile_fields(st.session_state.train_shp_path)
        if fields:
            fields = [f for f in fields if f.lower() != 'geometry']
            
            class_attr = st.sidebar.selectbox("ç±»åˆ«ç¼–å·å­—æ®µ", fields, key="class_attr")
            name_attr = st.sidebar.selectbox("ç±»åˆ«åç§°å­—æ®µ", fields, key="name_attr")
        else:
            st.sidebar.error("æ— æ³•è¯»å–shapefileå­—æ®µ")
            class_attr = st.sidebar.text_input("ç±»åˆ«ç¼–å·å­—æ®µ", key="class_attr")
            name_attr = st.sidebar.text_input("ç±»åˆ«åç§°å­—æ®µ", key="name_attr")
    else:
        class_attr = st.sidebar.text_input("ç±»åˆ«ç¼–å·å­—æ®µ", key="class_attr")
        name_attr = st.sidebar.text_input("ç±»åˆ«åç§°å­—æ®µ", key="name_attr")
    
    # èƒŒæ™¯å€¼è®¾ç½®
    st.sidebar.subheader("ğŸ¨ èƒŒæ™¯å€¼è®¾ç½®")
    ignore_background = st.sidebar.checkbox("å¿½ç•¥èƒŒæ™¯å€¼", value=True, key="ignore_bg")
    background_value = st.sidebar.number_input("èƒŒæ™¯å€¼", value=0, key="bg_value")
    
    # åˆ†ç±»å‚æ•°
    st.sidebar.subheader("âš™ï¸ åˆ†ç±»å‚æ•°")
    n_estimators = st.sidebar.slider("æ ‘æ¨¡å‹æ•°é‡", 10, 500, 100, key="n_estimators")
    block_size = st.sidebar.selectbox("åˆ†å—å¤§å°", [256, 512, 1024, 2048], index=1, key="block_size")
    
    # æ€§èƒ½ä¼˜åŒ–
    st.sidebar.subheader("âš¡ æ€§èƒ½ä¼˜åŒ–")
    enable_sampling = st.sidebar.checkbox("å¯ç”¨é‡‡æ ·", value=True, key="enable_sampling")
    max_samples = st.sidebar.slider("æœ€å¤§æ ·æœ¬æ•°", 10000, 200000, 50000, step=10000, key="max_samples")
    fast_mode = st.sidebar.checkbox("å¿«é€Ÿæ¨¡å¼", value=False, key="fast_mode")
    
    # åˆ†ç±»å™¨é€‰æ‹©
    st.sidebar.subheader("ğŸ¤– åˆ†ç±»å™¨é€‰æ‹©")
    all_classifiers = st.session_state.backend.get_all_classifiers()
    
    # åˆ†ç±»å™¨åˆ†ç»„
    svm_codes = ["svm_linear", "linear_svc", "sgd_svm", "nystroem_svm", "rbf_sampler_svm", "svm_rbf"]
    tree_codes = ["rf", "et", "dt", "xgb", "lgb", "gb", "ada"]
    other_codes = ["knn", "nb", "lr", "mlp"]
    
    selected_classifiers = []
    
    # SVMåˆ†ç±»å™¨
    st.sidebar.markdown("**SVMåˆ†ç±»å™¨:**")
    for code in svm_codes:
        if code in all_classifiers:
            _, name, _, _, _, _ = all_classifiers[code]
            if st.sidebar.checkbox(name, key=f"clf_{code}"):
                selected_classifiers.append(code)
    
    # æ ‘æ¨¡å‹åˆ†ç±»å™¨
    st.sidebar.markdown("**æ ‘æ¨¡å‹åˆ†ç±»å™¨:**")
    for code in tree_codes:
        if code in all_classifiers:
            _, name, _, _, _, _ = all_classifiers[code]
            if st.sidebar.checkbox(name, key=f"clf_{code}"):
                selected_classifiers.append(code)
    
    # å…¶ä»–åˆ†ç±»å™¨
    st.sidebar.markdown("**å…¶ä»–åˆ†ç±»å™¨:**")
    for code in other_codes:
        if code in all_classifiers:
            _, name, _, _, _, _ = all_classifiers[code]
            if st.sidebar.checkbox(name, key=f"clf_{code}"):
                selected_classifiers.append(code)
    
    # å¿«æ·é€‰æ‹©æŒ‰é’®
    st.sidebar.subheader("å¿«æ·é€‰æ‹©")
    col1, col2, col3 = st.sidebar.columns(3)
    
    with col1:
        if st.button("å…¨é€‰", key="select_all"):
            for code in all_classifiers.keys():
                st.session_state[f"clf_{code}"] = True
    
    with col2:
        if st.button("æ¨è", key="select_recommended"):
            recommended = ["rf", "xgb", "et", "lgb", "linear_svc", "nystroem_svm"]
            for code in all_classifiers.keys():
                st.session_state[f"clf_{code}"] = (code in recommended)
    
    with col3:
        if st.button("å¿«é€Ÿ", key="select_fast"):
            fast = ["rf", "et", "dt", "xgb", "lgb", "nb", "lr", "sgd_svm", "linear_svc"]
            for code in all_classifiers.keys():
                st.session_state[f"clf_{code}"] = (code in fast)
    
    # ä¸»å†…å®¹åŒºåŸŸ
    tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“ è¿è¡Œæ§åˆ¶", "ğŸ“Š ç²¾åº¦å¯¹æ¯”", "ğŸ”¥ æ··æ·†çŸ©é˜µ", "â±ï¸ æ—¶é—´å¯¹æ¯”", "ğŸ—ºï¸ ç»“æœé¢„è§ˆ"])
    
    with tab1:
        st.header("è¿è¡Œæ§åˆ¶")
        
        # è¿è¡ŒæŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col1:
            if st.button("â–¶ å¼€å§‹åˆ†ç±»", type="primary", disabled=st.session_state.is_running):
                if not image_file:
                    st.error("è¯·é€‰æ‹©å½±åƒæ–‡ä»¶ï¼")
                elif 'train_shp_path' not in st.session_state:
                    st.error("è¯·ä¸Šä¼ è®­ç»ƒæ ·æœ¬ZIPæ–‡ä»¶ï¼")
                elif not class_attr:
                    st.error("è¯·é€‰æ‹©ç±»åˆ«ç¼–å·å­—æ®µï¼")
                elif not selected_classifiers:
                    st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªåˆ†ç±»å™¨ï¼")
                else:
                    # ä¿å­˜åˆ†ç±»å‚æ•°
                    st.session_state.classification_params = {
                        'image_file': image_file,
                        'class_attr': class_attr,
                        'name_attr': name_attr,
                        'ignore_background': ignore_background,
                        'background_value': background_value,
                        'n_estimators': n_estimators,
                        'block_size': block_size,
                        'enable_sampling': enable_sampling,
                        'max_samples': max_samples,
                        'fast_mode': fast_mode,
                        'selected_classifiers': selected_classifiers
                    }
                    
                    st.session_state.is_running = True
                    st.session_state.log_messages = []
                    st.session_state.comparison_results = []
                    st.session_state.comparison_df = None
                    st.session_state.current_classifier_index = 0
                    st.session_state.selected_classifiers_list = selected_classifiers.copy()
                    
                    add_log("ğŸš€ å¼€å§‹åˆ†ç±»ä»»åŠ¡...")
                    add_log(f"ğŸ“‹ é€‰æ‹©çš„åˆ†ç±»å™¨: {', '.join([all_classifiers[code][1] for code in selected_classifiers])}")
        
        with col2:
            if st.button("â¸ åœæ­¢", disabled=not st.session_state.is_running):
                st.session_state.is_running = False
                add_log("â¹ï¸ ç”¨æˆ·æ‰‹åŠ¨åœæ­¢åˆ†ç±»ä»»åŠ¡")
        
        with col3:
            if st.session_state.comparison_df is not None and not st.session_state.comparison_df.empty:
                if st.button("ğŸ“¥ ä¸‹è½½ç»“æœ"):
                    download_results()
        
        # è¿›åº¦æ˜¾ç¤º
        if st.session_state.progress > 0:
            st.progress(st.session_state.progress)
            if (st.session_state.current_classifier_index > 0 and 
                st.session_state.selected_classifiers_list):
                total_classifiers = len(st.session_state.selected_classifiers_list)
                current_index = st.session_state.current_classifier_index
                current_classifier = st.session_state.selected_classifiers_list[current_index - 1]
                all_classifiers = st.session_state.backend.get_all_classifiers()
                classifier_name = all_classifiers[current_classifier][1] if current_classifier in all_classifiers else current_classifier
                st.write(f"æ­£åœ¨å¤„ç†: {classifier_name} ({current_index}/{total_classifiers})")
        
        # æ—¥å¿—æ˜¾ç¤º
        st.subheader("è¿è¡Œæ—¥å¿—")
        log_container = st.container()
        with log_container:
            for message in st.session_state.log_messages:
                st.text(message)
        
        # å¦‚æœæ­£åœ¨è¿è¡Œï¼Œæ‰§è¡Œåˆ†ç±»ä»»åŠ¡
        if st.session_state.is_running and st.session_state.selected_classifiers_list:
            if st.session_state.current_classifier_index < len(st.session_state.selected_classifiers_list):
                # æ‰§è¡Œä¸‹ä¸€ä¸ªåˆ†ç±»å™¨
                run_classification_step()
            else:
                # æ‰€æœ‰åˆ†ç±»å™¨å®Œæˆ
                st.session_state.is_running = False
                add_log("âœ… æ‰€æœ‰åˆ†ç±»å™¨å¤„ç†å®Œæˆ!")
                st.success("æ‰€æœ‰åˆ†ç±»å™¨å¤„ç†å®Œæˆ!")
    
    with tab2:
        st.header("ç²¾åº¦å¯¹æ¯”")
        if st.session_state.comparison_df is not None and not st.session_state.comparison_df.empty:
            display_accuracy_comparison()
        else:
            st.info("æš‚æ— åˆ†ç±»ç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†ç±»")
    
    with tab3:
        st.header("æ··æ·†çŸ©é˜µ")
        if (st.session_state.comparison_df is not None and not st.session_state.comparison_df.empty and 
            'best_confusion_matrix' in st.session_state and st.session_state.best_confusion_matrix is not None):
            display_confusion_matrix()
        else:
            st.info("æš‚æ— æ··æ·†çŸ©é˜µæ•°æ®")
    
    with tab4:
        st.header("æ—¶é—´å¯¹æ¯”")
        if st.session_state.comparison_df is not None and not st.session_state.comparison_df.empty:
            display_time_comparison()
        else:
            st.info("æš‚æ— æ—¶é—´å¯¹æ¯”æ•°æ®")
    
    with tab5:
        st.header("åˆ†ç±»ç»“æœé¢„è§ˆ")
        if (st.session_state.comparison_df is not None and not st.session_state.comparison_df.empty and 
            'best_result_path' in st.session_state and st.session_state.best_result_path is not None):
            display_result_preview()
        else:
            st.info("æš‚æ— åˆ†ç±»ç»“æœé¢„è§ˆ")
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if st.sidebar.button("æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
        if 'train_temp_dir' in st.session_state:
            safe_delete_temp_dir(st.session_state.train_temp_dir)
            st.session_state.train_temp_dir = None
            st.session_state.train_shp_path = None
        
        if 'val_temp_dir' in st.session_state:
            safe_delete_temp_dir(st.session_state.val_temp_dir)
            st.session_state.val_temp_dir = None
            st.session_state.val_shp_path = None
        
        st.sidebar.success("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†!")

def run_classification_step():
    """æ‰§è¡Œå•ä¸ªåˆ†ç±»å™¨çš„åˆ†ç±»ä»»åŠ¡"""
    
    backend = st.session_state.backend
    params = st.session_state.classification_params
    selected_classifiers = st.session_state.selected_classifiers_list
    current_index = st.session_state.current_classifier_index
    
    if current_index >= len(selected_classifiers):
        return
    
    clf_code = selected_classifiers[current_index]
    all_classifiers = backend.get_all_classifiers()
    
    if clf_code not in all_classifiers:
        add_log(f"âŒ æœªçŸ¥çš„åˆ†ç±»å™¨ä»£ç : {clf_code}")
        st.session_state.current_classifier_index += 1
        st.rerun()
        return
    
    clf, clf_name, clf_desc, needs_encoding, needs_scaling, speed_tag = all_classifiers[clf_code]
    
    add_log(f"\n{'='*80}")
    add_log(f"[{current_index+1}/{len(selected_classifiers)}] {clf_name}")
    add_log(f"{'='*80}")
    
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œåˆå§‹åŒ–ä¸´æ—¶ç›®å½•å’Œæ•°æ®
    if current_index == 0:
        # åˆ›å»ºä¸»ä¸´æ—¶ç›®å½•
        main_temp_dir = tempfile.mkdtemp()
        st.session_state.main_temp_dir = main_temp_dir
        
        try:
            temp_path = Path(main_temp_dir)
            
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
            image_path = temp_path / "image.tif"
            with open(image_path, 'wb') as f:
                f.write(params['image_file'].getvalue())
            
            # ä½¿ç”¨session stateä¸­ä¿å­˜çš„shapefileè·¯å¾„
            train_shp_path = st.session_state.train_shp_path
            val_shp_path = st.session_state.val_shp_path if 'val_shp_path' in st.session_state else None
            
            # è¯»å–å½±åƒ
            add_log("ğŸ“ è¯»å–å½±åƒ...")
            try:
                # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿æ–‡ä»¶æ­£ç¡®å…³é—­
                with rxr.open_rasterio(image_path, masked=True) as img:
                    # å°†æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­
                    img_data = img.load()
                    n_pixels = img_data.shape[1] * img_data.shape[2]
                    add_log(f"   å°ºå¯¸: {img_data.shape[1]}Ã—{img_data.shape[2]} = {n_pixels:,} åƒå…ƒ")
                    add_log(f"   æ³¢æ®µæ•°: {img_data.shape[0]}")
                    add_log(f"   æ•°æ®ç±»å‹: {img_data.dtype}")
                    add_log(f"   åæ ‡ç³»ç»Ÿ: {img_data.rio.crs}")
                    
                    # è¯»å–ç±»åˆ«ä¿¡æ¯
                    add_log("ğŸ“Š è¯»å–ç±»åˆ«ä¿¡æ¯...")
                    class_names, class_colors, _ = backend.get_class_info_from_shp(
                        train_shp_path, params['class_attr'], params['name_attr']
                    )
                    st.session_state.class_names = class_names
                    st.session_state.class_colors = class_colors
                    add_log(f"   ç±»åˆ«: {list(class_names.values())}")
                    
                    # æå–è®­ç»ƒæ ·æœ¬
                    add_log("ğŸ¯ å¤„ç†è®­ç»ƒæ ·æœ¬...")
                    train_mask = backend.rasterize_samples(train_shp_path, img_data, params['class_attr'])
                    
                    max_samples_val = params['max_samples'] if params['enable_sampling'] else None
                    
                    X_train, y_train, n_nan, n_inf, n_sampled = backend.extract_samples(
                        img_data, train_mask, 
                        ignore_background=params['ignore_background'],
                        background_value=params['background_value'],
                        max_samples=max_samples_val
                    )
                    
                    add_log(f"   è®­ç»ƒæ ·æœ¬æ•°: {len(y_train):,}")
                    if n_nan > 0:
                        add_log(f"   â””â”€ ç§»é™¤NaN: {n_nan:,}")
                    if n_inf > 0:
                        add_log(f"   â””â”€ ç§»é™¤Inf: {n_inf:,}")
                    if n_sampled > 0:
                        add_log(f"   â””â”€ é‡‡æ ·å‡å°‘: {n_sampled:,}")
                    
                    # æ£€æŸ¥æ ·æœ¬æ•°é‡
                    if len(y_train) == 0:
                        add_log("âŒ é”™è¯¯: æ²¡æœ‰æå–åˆ°è®­ç»ƒæ ·æœ¬!")
                        st.session_state.is_running = False
                        return
                    
                    # æå–éªŒè¯æ ·æœ¬
                    val_exists = val_shp_path and os.path.exists(val_shp_path)
                    yv_true = None
                    valid_val = None
                    if val_exists:
                        add_log("âœ… å¤„ç†éªŒè¯æ ·æœ¬...")
                        val_mask = backend.rasterize_samples(val_shp_path, img_data, params['class_attr'])
                        
                        if params['ignore_background']:
                            background_mask = backend.get_background_mask(img_data, params['background_value'])
                            valid_val = (val_mask > 0) & (~background_mask)
                        else:
                            valid_val = val_mask > 0
                        
                        yv_true = val_mask[valid_val]
                        add_log(f"   éªŒè¯æ ·æœ¬æ•°: {len(yv_true):,}")
                    
                    # ä¿å­˜æ•°æ®åˆ°session state
                    st.session_state.temp_path = temp_path
                    st.session_state.image_path = image_path
                    st.session_state.img_data = img_data
                    st.session_state.X_train = X_train
                    st.session_state.y_train = y_train
                    st.session_state.val_exists = val_exists
                    st.session_state.yv_true = yv_true
                    st.session_state.valid_val = valid_val
                    
            except Exception as e:
                add_log(f"âŒ å½±åƒè¯»å–é”™è¯¯: {str(e)}")
                st.session_state.is_running = False
                return
            
        except Exception as e:
            add_log(f"\nâŒ é”™è¯¯: {str(e)}")
            st.session_state.is_running = False
            return
    
    # å¤„ç†å½“å‰åˆ†ç±»å™¨
    try:
        temp_path = st.session_state.temp_path
        img_data = st.session_state.img_data
        X_train = st.session_state.X_train
        y_train = st.session_state.y_train
        val_exists = st.session_state.val_exists
        yv_true = st.session_state.yv_true
        valid_val = st.session_state.valid_val
        
        clf_dir = temp_path / clf_code
        clf_dir.mkdir(exist_ok=True)
        
        # æ•°æ®é¢„å¤„ç†
        label_encoder = None
        scaler = None
        X_train_use = X_train.copy()
        y_train_use = y_train.copy()
        
        if needs_encoding:
            add_log("   ğŸ”„ æ ‡ç­¾ç¼–ç ...")
            label_encoder = LabelEncoder()
            y_train_use = label_encoder.fit_transform(y_train)
        
        if needs_scaling:
            add_log("   ğŸ“ ç‰¹å¾ç¼©æ”¾...")
            scaler = StandardScaler()
            X_train_use = scaler.fit_transform(X_train_use)
        
        # è®­ç»ƒ
        add_log("   ğŸ”¨ è®­ç»ƒä¸­...")
        train_start = time.time()
        clf.fit(X_train_use, y_train_use)
        train_time = time.time() - train_start
        add_log(f"   âœ“ è®­ç»ƒå®Œæˆ: {train_time:.2f}ç§’")
        
        # è®­ç»ƒé›†ç²¾åº¦
        y_train_pred = clf.predict(X_train_use)
        
        if label_encoder is not None:
            y_train_pred = label_encoder.inverse_transform(y_train_pred)
        
        train_metrics = backend.calculate_metrics(y_train, y_train_pred)
        add_log(f"   ğŸ“ˆ è®­ç»ƒé›† - ç²¾åº¦: {train_metrics['overall_accuracy']:.4f}")
        
        if not st.session_state.is_running:
            return
        
        # é¢„æµ‹æ•´å¹…å½±åƒ
        add_log("   ğŸ—ºï¸  é¢„æµ‹å½±åƒ...")
        
        def update_progress(progress):
            st.session_state.progress = progress
        
        pred_start = time.time()
        classified_path = clf_dir / f"classified_{clf_code}.tif"
        
        backend.predict_by_block(
            clf, img_data, classified_path, 
            block_size=params['block_size'],
            ignore_background=params['ignore_background'],
            background_value=params['background_value'],
            progress_callback=update_progress,
            label_encoder=label_encoder,
            scaler=scaler
        )
        
        pred_time = time.time() - pred_start
        add_log(f"   âœ“ é¢„æµ‹å®Œæˆ: {pred_time:.2f}ç§’")
        
        # éªŒè¯é›†ç²¾åº¦
        val_metrics = {'overall_accuracy': np.nan, 'kappa': np.nan}
        yv_pred = None
        
        if val_exists and yv_true is not None:
            # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è¯»å–é¢„æµ‹ç»“æœ
            with rxr.open_rasterio(classified_path) as pred_img:
                pred_arr = pred_img.values.squeeze()
            
            yv_pred = pred_arr[valid_val]
            val_metrics = backend.calculate_metrics(yv_true, yv_pred)
            add_log(f"   ğŸ“Š éªŒè¯é›† - ç²¾åº¦: {val_metrics['overall_accuracy']:.4f}")
            
            # è®°å½•æœ€ä½³åˆ†ç±»å™¨
            if ('best_accuracy' not in st.session_state or 
                val_metrics['overall_accuracy'] > st.session_state.best_accuracy):
                st.session_state.best_accuracy = val_metrics['overall_accuracy']
                st.session_state.best_result_path = classified_path
                
                # ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®
                cm = confusion_matrix(yv_true, yv_pred)
                st.session_state.best_confusion_matrix = cm
                st.session_state.best_y_true = yv_true
                st.session_state.best_y_pred = yv_pred
        
        # è®°å½•ç»“æœ
        result = {
            'åˆ†ç±»å™¨ä»£ç ': clf_code,
            'åˆ†ç±»å™¨åç§°': clf_name,
            'è®­ç»ƒé›†ç²¾åº¦': train_metrics['overall_accuracy'],
            'è®­ç»ƒé›†Kappa': train_metrics['kappa'],
            'éªŒè¯é›†ç²¾åº¦': val_metrics['overall_accuracy'],
            'éªŒè¯é›†Kappa': val_metrics['kappa'],
            'è®­ç»ƒæ—¶é—´(ç§’)': train_time,
            'é¢„æµ‹æ—¶é—´(ç§’)': pred_time,
        }
        
        # æ›´æ–°æ¯”è¾ƒç»“æœ
        if st.session_state.comparison_results is None:
            st.session_state.comparison_results = []
        
        st.session_state.comparison_results.append(result)
        st.session_state.comparison_df = pd.DataFrame(st.session_state.comparison_results)
        
        add_log(f"   âœ… {clf_name} å®Œæˆ!")
        
    except Exception as e:
        add_log(f"   âŒ {clf_name} å¤±è´¥: {str(e)}")
        import traceback
        add_log(traceback.format_exc())
    
    # æ›´æ–°è¿›åº¦å¹¶ç»§ç»­ä¸‹ä¸€ä¸ªåˆ†ç±»å™¨
    st.session_state.current_classifier_index += 1
    st.session_state.progress = st.session_state.current_classifier_index / len(selected_classifiers)
    
    # è‡ªåŠ¨åˆ·æ–°ç•Œé¢
    st.rerun()

# å…¶ä»–æ˜¾ç¤ºå‡½æ•°ä¿æŒä¸å˜...
def display_accuracy_comparison():
    """æ˜¾ç¤ºç²¾åº¦å¯¹æ¯”"""
    if st.session_state.comparison_df is None or st.session_state.comparison_df.empty:
        st.info("æš‚æ— åˆ†ç±»ç»“æœï¼Œè¯·å…ˆè¿è¡Œåˆ†ç±»")
        return
    
    df = st.session_state.comparison_df
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # ç²¾åº¦å¯¹æ¯”
    x = np.arange(len(df))
    width = 0.35
    
    ax1.bar(x - width/2, df['è®­ç»ƒé›†ç²¾åº¦'], width, label='è®­ç»ƒé›†', alpha=0.8, color='steelblue')
    ax1.bar(x + width/2, df['éªŒè¯é›†ç²¾åº¦'], width, label='éªŒè¯é›†', alpha=0.8, color='coral')
    
    ax1.set_xlabel('åˆ†ç±»å™¨', fontsize=11)
    ax1.set_ylabel('ç²¾åº¦', fontsize=11)
    ax1.set_title('æ€»ä½“ç²¾åº¦å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(df['åˆ†ç±»å™¨åç§°'], rotation=45, ha='right', fontsize=9)
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')
    ax1.set_ylim([0, 1.05])
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (train_acc, val_acc) in enumerate(zip(df['è®­ç»ƒé›†ç²¾åº¦'], df['éªŒè¯é›†ç²¾åº¦'])):
        ax1.text(i - width/2, train_acc + 0.01, f'{train_acc:.3f}', 
                ha='center', va='bottom', fontsize=8)
        ax1.text(i + width/2, val_acc + 0.01, f'{val_acc:.3f}', 
                ha='center', va='bottom', fontsize=8)
    
    # Kappaå¯¹æ¯”
    ax2.bar(x - width/2, df['è®­ç»ƒé›†Kappa'], width, label='è®­ç»ƒé›†', alpha=0.8, color='steelblue')
    ax2.bar(x + width/2, df['éªŒè¯é›†Kappa'], width, label='éªŒè¯é›†', alpha=0.8, color='coral')
    
    ax2.set_xlabel('åˆ†ç±»å™¨', fontsize=11)
    ax2.set_ylabel('Kappaç³»æ•°', fontsize=11)
    ax2.set_title('Kappaç³»æ•°å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(df['åˆ†ç±»å™¨åç§°'], rotation=45, ha='right', fontsize=9)
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')
    ax2.set_ylim([0, 1.05])
    
    plt.tight_layout()
    st.pyplot(fig)
    
    # æ˜¾ç¤ºè¯¦ç»†æ•°æ®
    st.subheader("è¯¦ç»†ç»“æœ")
    st.dataframe(df)

def display_confusion_matrix():
    """æ˜¾ç¤ºæ··æ·†çŸ©é˜µ"""
    if (st.session_state.comparison_df is None or st.session_state.comparison_df.empty or 
        'best_confusion_matrix' not in st.session_state or st.session_state.best_confusion_matrix is None):
        st.info("æš‚æ— æ··æ·†çŸ©é˜µæ•°æ®")
        return
    
    cm = st.session_state.best_confusion_matrix
    class_names = list(st.session_state.class_names.values())
    
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # ç»˜åˆ¶çƒ­å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'æ ·æœ¬æ•°é‡'}, ax=ax)
    
    ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=11)
    ax.set_ylabel('çœŸå®ç±»åˆ«', fontsize=11)
    ax.set_title('æœ€ä½³åˆ†ç±»å™¨æ··æ·†çŸ©é˜µ', fontsize=12, fontweight='bold')
    
    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
    plt.setp(ax.get_yticklabels(), rotation=0)
    
    plt.tight_layout()
    st.pyplot(fig)

def display_time_comparison():
    """æ˜¾ç¤ºæ—¶é—´å¯¹æ¯”"""
    if st.session_state.comparison_df is None or st.session_state.comparison_df.empty:
        st.info("æš‚æ— æ—¶é—´å¯¹æ¯”æ•°æ®")
        return
    
    df = st.session_state.comparison_df
    
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(df))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, df['è®­ç»ƒæ—¶é—´(ç§’)'], width, label='è®­ç»ƒæ—¶é—´', 
                  alpha=0.8, color='lightgreen')
    bars2 = ax.bar(x + width/2, df['é¢„æµ‹æ—¶é—´(ç§’)'], width, label='é¢„æµ‹æ—¶é—´', 
                  alpha=0.8, color='lightcoral')
    
    ax.set_xlabel('åˆ†ç±»å™¨', fontsize=11)
    ax.set_ylabel('æ—¶é—´ (ç§’)', fontsize=11)
    ax.set_title('è®­ç»ƒå’Œé¢„æµ‹æ—¶é—´å¯¹æ¯”', fontsize=12, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(df['åˆ†ç±»å™¨åç§°'], rotation=45, ha='right', fontsize=9)
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}s', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    st.pyplot(fig)

def display_result_preview():
    """æ˜¾ç¤ºåˆ†ç±»ç»“æœé¢„è§ˆ"""
    if (st.session_state.comparison_df is None or st.session_state.comparison_df.empty or 
        'best_result_path' not in st.session_state or st.session_state.best_result_path is None):
        st.info("æš‚æ— åˆ†ç±»ç»“æœé¢„è§ˆ")
        return
    
    try:
        best_result_path = st.session_state.best_result_path
        class_names = st.session_state.class_names
        class_colors = st.session_state.class_colors
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦å¤„ç†å½±åƒæ˜¾ç¤º
        st.info("åˆ†ç±»ç»“æœé¢„è§ˆåŠŸèƒ½åœ¨Webç‰ˆæœ¬ä¸­å—é™")
        st.write(f"æœ€ä½³åˆ†ç±»ç»“æœæ–‡ä»¶: {best_result_path.name}")
        st.write("ç±»åˆ«ä¿¡æ¯:", class_names)
        
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¾ç¤ºåˆ†ç±»ç»“æœçš„ç¼©ç•¥å›¾
        # ä½†ç”±äºWebç¯å¢ƒé™åˆ¶ï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
        
    except Exception as e:
        st.error(f"é¢„è§ˆæ˜¾ç¤ºé”™è¯¯: {str(e)}")

def download_results():
    """ä¸‹è½½ç»“æœæ–‡ä»¶"""
    if st.session_state.comparison_df is None or st.session_state.comparison_df.empty:
        st.error("æ²¡æœ‰å¯ä¸‹è½½çš„ç»“æœ")
        return
    
    # åˆ›å»ºä¸‹è½½æ–‡ä»¶
    csv = st.session_state.comparison_df.to_csv(index=False)
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½CSVç»“æœ",
        data=csv,
        file_name="classification_results.csv",
        mime="text/csv"
    )
    
    # ç”ŸæˆæŠ¥å‘Šæ–‡æœ¬
    report = f"""é¥æ„Ÿå½±åƒåˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
================================
æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
è®­ç»ƒæ ·æœ¬: {len(st.session_state.y_train):,}
æ€»è€—æ—¶: è¯·æŸ¥çœ‹å…·ä½“åˆ†ç±»å™¨æ—¶é—´

éªŒè¯é›†ç²¾åº¦æ’å:
"""
    
    sorted_df = st.session_state.comparison_df.sort_values('éªŒè¯é›†ç²¾åº¦', ascending=False)
    for idx, (_, row) in enumerate(sorted_df.iterrows(), 1):
        report += f"{idx}. {row['åˆ†ç±»å™¨åç§°']:15s} - ç²¾åº¦: {row['éªŒè¯é›†ç²¾åº¦']:.4f}\n"
    
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½æ–‡æœ¬æŠ¥å‘Š",
        data=report,
        file_name="classification_report.txt",
        mime="text/plain"
    )

if __name__ == "__main__":
    main()